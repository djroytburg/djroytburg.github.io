{
  "roytburg2025-words-action": {
    "abstract": "In the wake of the 2024 US presidential election, pundits on both the left and the right pointed to a conservative backlash against 'woke politics' to explain the election's outcome. These politics, rooted in substantive beliefs about equity and justice--and particularly racial justice--owe their most recent rise to prominence to the Black Lives Matter (BLM) movement. A significant body of work, both qualitative and quantitative, has documented how BLM was able to move these beliefs from the margin to the mainstream. In this paper, we focus on the words that index these beliefs, devising a novel method of modeling semantic leadership across a set of communities associated with the BLM movement that is informed by domain-specific theory about Black Twitter. We describe our bespoke approaches to time-binning, community clustering, and connecting communities over time, as well as our adaptation of state-of-the-art approaches to semantic change detection and semantic leadership induction. We find evidence at scale of the leadership role of BLM activists and progressives, as well as of Black celebrities. We also find evidence of sustained conservative engagement with this discourse, suggesting an alternative explanation for how we have arrived at the present political moment.",
    "paper_url": "https://ojs.aaai.org/index.php/ICWSM/article/view/35895",
    "slides_url": "https://docs.google.com/presentation/d/1FoaRlw_MD0vldCzs35ji4YOLH-jxCnFIkrMhW4LmH7Q/edit?usp=sharing",
    "code_url": null,
    "figure": "static/figures/words-action.png",
    "equal_contribution": [0, 1]
  },
  "roytburg2025-break-mirror": {
    "abstract": "Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from 'self-preference bias': a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.",
    "paper_url": "https://arxiv.org/abs/2509.03647",
    "slides_url": null,
    "code_url": "https://github.com/djroytburg/steering_self_preference",
    "figure": "static/figures/break-mirror.png",
    "equal_contribution": [0, 1, 3, 4],
    "also_at": [
      "NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle",
      "NeurIPS 2025 Workshop on Reliable ML from Unreliable Data"
    ]
  },
  "roytburg2025-generative": {
    "abstract": "This thesis introduces a generative approach to argument mining using pretrained language models. Unlike classical discriminative methods requiring complex, task-specific architectures, we adapt a structured annotation scheme, allowing models to translate raw text inputs directly to labeled, structured outputs. We systematically evaluate this approach across model architectures, labeling strategies, and training paradigms (fine-tuning vs. few-shot learning). Our generative models significantly outperform previous generation-based baselines by 10.41% for relations and 5.28% for components. We introduce compliance metrics to analyze model failure modes, revealing challenges in entity coherence and label hallucination, and provide key insights into the optimal use of encoder-decoder and decoder-only architectures for high-fidelity argumentative structure prediction.",
    "paper_url": "https://etd.library.emory.edu/concern/etds/5t34sm11w?locale=en",
    "slides_url": null,
    "code_url": null,
    "equal_contribution": null
  }
}
